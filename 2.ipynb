{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Deep neural network (Any One from the following)\n",
    "## 1. Multiclass classification using Deep Neural Networks: Example: Use the OCR letter recognition dataset https://archive.ics.uci.edu/ml/datasets/letter+recognition\n",
    "## 2. Binary classification using Deep Neural Networks Example: Classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews. Use IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewers has mentioned that ...          1\n",
       "1  a wonderful little production. <br /><br />the...          1\n",
       "2  i thought this was a wonderful way to spend ti...          1\n",
       "3  basically there's a family where a little boy ...          0\n",
       "4  petter mattei's \"love in the time of money\" is...          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "raw_data_df = pd.read_csv('IMDB_Dataset.csv')\n",
    "raw_data_df['sentiment'] = raw_data_df['sentiment'].apply(lambda row : 1 if row == 'positive' else 0)\n",
    "raw_data_df['review'] = raw_data_df['review'].apply(lambda row : row.lower())\n",
    "\n",
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = raw_data_df['review'].to_numpy()\n",
    "labels = raw_data_df['sentiment'].to_numpy()\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.4, random_state = 0)\n",
    "features_valid, features_test, labels_valid, labels_test = train_test_split(features_test, labels_test, test_size=0.5, random_state=0)\n",
    "features_train = tf.convert_to_tensor(features_train)\n",
    "labels_train = tf.convert_to_tensor(labels_train)\n",
    "features_valid = tf.convert_to_tensor(features_valid)\n",
    "labels_valid = tf.convert_to_tensor(labels_valid)\n",
    "features_test = tf.convert_to_tensor(features_test)\n",
    "labels_test = tf.convert_to_tensor(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'liked stanley & iris very much. acting was very good. story had a unique and interesting arrangement. the absence of violence and porno sex was refreshing. characters were very convincing and felt like you could understand their feelings. very enjoyable movie.'>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds = tf.data.Dataset.from_tensor_slices((features_valid, labels_valid))\n",
    "next(iter(valid_ds))\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((features_valid, labels_valid))\n",
    "next(iter(valid_ds))\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((features_test, labels_test))\n",
    "next(iter(test_ds))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((features_train, labels_train))\n",
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = train_ds.batch(batch_size=BATCH_SIZE)\n",
    "train_ds.cardinality()\n",
    "\n",
    "train_ds = train_ds.batch(batch_size=BATCH_SIZE)\n",
    "train_ds.cardinality()\n",
    "valid_ds = valid_ds.batch(batch_size=BATCH_SIZE)\n",
    "train_ds.cardinality()\n",
    "test_ds = test_ds.batch(batch_size=BATCH_SIZE)\n",
    "test_ds.cardinality()\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  std_text = tf.strings.lower(input_data)#remove any urls from the text\n",
    "  std_text = tf.strings.regex_replace(std_text, r\"https:\\/\\/.*[\\r\\n]*\", '')\n",
    "  std_text = tf.strings.regex_replace(std_text, r\"www\\.\\w*\\.\\w\\w\\w\", '')\n",
    "  std_text = tf.strings.regex_replace(std_text, r\"<[\\w]*[\\s]*/>\", '')\n",
    "  std_text = tf.strings.regex_replace(std_text, '[%s]' % re.escape(string.punctuation), '')\n",
    "  std_text = tf.strings.regex_replace(std_text, '\\s{2}', '')\n",
    "  std_text = tf.strings.strip(std_text)\n",
    "  return std_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\", line 475, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Rahul\\College SEM 8\\DL\\Implement\\2.ipynb Cell 6\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vectorizer_layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mTextVectorization(max_tokens\u001b[39m=\u001b[39mVOCAB_SIZE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                                 standardize\u001b[39m=\u001b[39mcustom_standardization)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                                                 \u001b[39m#output_mode='tf_idf')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m vectorizer_layer\u001b[39m.\u001b[39;49madapt(train_ds\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m text, label: text))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m vocab \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(vectorizer_layer\u001b[39m.\u001b[39mget_vocabulary())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Rahul/College%20SEM%208/DL/Implement/2.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m examples, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_ds\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:472\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madapt\u001b[39m(\u001b[39mself\u001b[39m, data, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m     \u001b[39m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[39m    Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49madapt(data, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps\u001b[39m=\u001b[39;49msteps)\n",
      "File \u001b[1;32mc:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m    257\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m--> 258\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[0;32m    259\u001b[0m         \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m    260\u001b[0m             context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecwr0vrey.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__adapt_step\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m      9\u001b[0m data \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mnext\u001b[39m), (ag__\u001b[39m.\u001b[39mld(iterator),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     10\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_adapt_maybe_build, (ag__\u001b[39m.\u001b[39mld(data),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mupdate_state, (ag__\u001b[39m.\u001b[39mld(data),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mc:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:475\u001b[0m, in \u001b[0;36mTextVectorization.update_state\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_state\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m--> 475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lookup_layer\u001b[39m.\u001b[39mupdate_state(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(data))\n",
      "File \u001b[1;32mc:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:573\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    572\u001b[0m     \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 573\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmust be 1. Received: inputs.shape=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith rank=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         )\n\u001b[0;32m    579\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m         inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\", line 475, in update_state  **\n        self._lookup_layer.update_state(self._preprocess(data))\n    File \"c:\\Users\\rahul-al\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 1000\n",
    "vectorizer_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                                                                standardize=custom_standardization)\n",
    "                                                                                #output_mode='tf_idf')\n",
    "\n",
    "vectorizer_layer.adapt(train_ds.map(lambda text, label: text))\n",
    "\n",
    "vocab = np.array(vectorizer_layer.get_vocabulary())\n",
    "\n",
    "examples, labels = next(iter(train_ds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 0.4419 - accuracy: 0.8093 - val_loss: 0.3160 - val_accuracy: 0.8719\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.2651 - accuracy: 0.8941 - val_loss: 0.2987 - val_accuracy: 0.8783\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 0.2266 - accuracy: 0.9120 - val_loss: 0.3117 - val_accuracy: 0.8748\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.2042 - accuracy: 0.9220 - val_loss: 0.3203 - val_accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.1884 - accuracy: 0.9297 - val_loss: 0.3331 - val_accuracy: 0.8726\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.1759 - accuracy: 0.9345 - val_loss: 0.3552 - val_accuracy: 0.8710\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 0.1657 - accuracy: 0.9385 - val_loss: 0.3750 - val_accuracy: 0.8665\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.1578 - accuracy: 0.9427 - val_loss: 0.4007 - val_accuracy: 0.8631\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.1494 - accuracy: 0.9453 - val_loss: 0.4046 - val_accuracy: 0.8627\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.1438 - accuracy: 0.9478 - val_loss: 0.4230 - val_accuracy: 0.8590\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4229661226272583, 0.859000027179718]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Read the dataset\n",
    "raw_data_df = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "# Convert sentiment labels to binary values\n",
    "raw_data_df['sentiment'] = raw_data_df['sentiment'].apply(lambda row: 1 if row == 'positive' else 0)\n",
    "\n",
    "# Convert reviews to lowercase\n",
    "raw_data_df['review'] = raw_data_df['review'].apply(lambda row: row.lower())\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data = raw_data_df.sample(frac=0.8, random_state=42)\n",
    "test_data = raw_data_df.drop(train_data.index)\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_data['review'])\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['review'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['review'])\n",
    "\n",
    "# Pad the sequences to ensure equal length\n",
    "max_length = 200  # or choose an appropriate maximum sequence length\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, truncating='post')\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # adjust as needed\n",
    "model.fit(train_padded, train_data['sentiment'], epochs=num_epochs, validation_data=(test_padded, test_data['sentiment']))\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(test_padded, test_data['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# sample_text = \"it was bad movie nothing to watch\"\n",
    "sample_text = \"such great movie loved must watch\"\n",
    "\n",
    "tokenizer_text = tokenizer.texts_to_sequences([sample_text])\n",
    "\n",
    "tokenizer_text = pad_sequences(tokenizer_text, maxlen=max_length, truncating='post')\n",
    "\n",
    "predictions = model.predict(tokenizer_text)\n",
    "\n",
    "if predictions[0][0] > 0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
